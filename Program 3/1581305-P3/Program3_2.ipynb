{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481a3047-1787-4d9b-80de-1e8fa9ff60a4",
   "metadata": {},
   "source": [
    "# Program 3: Audio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb12f5e-9cf1-4981-a487-6bdeca94e707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchsummary in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /WAVE/apps2/el8/conda/envs/JupyterHub/20240807-CUDA/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /WAVE/apps2/el8/conda/envs/JupyterHub/20240807-CUDA/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: triton==3.2.0 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /WAVE/archive/users/emitchell/.local/lib/python3.9/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /WAVE/apps2/el8/conda/envs/JupyterHub/20240807-CUDA/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio torchsummary --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25859784-16e8-4720-9737-48275a7c9de3",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc1c5e4-04b9-4905-be10-8205097654bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef71d9-270e-400c-9b88-520a2d37ddcf",
   "metadata": {},
   "source": [
    "## Creating Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301eaa4c-0111-4f2c-93c0-a2640b631a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, wav_directory, transformation, target_sample_rate, num_samples, device, label_file_path=None):\n",
    "        \n",
    "        if label_file_path is None:\n",
    "            self.labels = None\n",
    "        else:\n",
    "            self.labels = pd.read_csv(label_file_path, header=None)   \n",
    "            \n",
    "        self.wav_directory = wav_directory\n",
    "        self.device = device\n",
    "        #register transformation onto the device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.labels is None:\n",
    "            return 0\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav_sample_path = self._get_wav_sample_path(index)\n",
    "        \n",
    "        signal, sample_rate = torchaudio.load(wav_sample_path)\n",
    "\n",
    "        #register signal onto the device\n",
    "        signal = signal.to(self.device)\n",
    "\n",
    "        #alter sample_rate to target_sample_rate\n",
    "        signal = self._resample(signal, sample_rate)\n",
    "        \n",
    "        #make signal into mono channel\n",
    "        signal = self._mix_down(signal)\n",
    "\n",
    "        ###ALTHOUGH IT IS GIVEN THAT EACH SIGNAL IS 5 SEC, CUT AND RIGHT PAD ENSURES THAT ALL AUDIO HAS THE SAME # of SAMPLES\n",
    "        #print(signal.shape)\n",
    "        \n",
    "        #cut if signal has more than expected num_samples\n",
    "        signal = self._cut(signal)\n",
    "\n",
    "        #right pad if signal has less than expected num_samples\n",
    "        signal = self._right_pad(signal)\n",
    "        \n",
    "        signal = self.transformation(signal)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return signal\n",
    "\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    def _get_wav_sample_path(self, index):\n",
    "        wav_file_name = f\"{int(index+1):05d}.wav\"\n",
    "        path = os.path.join(self.wav_directory, wav_file_name)\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        if self.labels is None:\n",
    "            return signal\n",
    "        return self.labels.iloc[index, 0]\n",
    "\n",
    "    def _resample(self, signal, sample_rate):\n",
    "        #only apply if sample rate is not equal to target sample rate\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down(self, signal):\n",
    "        #only apply if signal has more than 1 channel\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _cut(self, signal):\n",
    "        #only apply if signal has more samples than num_samples\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "        \n",
    "    def _right_pad(self, signal):\n",
    "        #only apply if signal has less samples than num_samples\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_zeros = self.num_samples - length_signal\n",
    "            #appends number of zeros that need to be filled on second axis of signal\n",
    "            last_dim_padding = (0, num_zeros)\n",
    "            signal = nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b263b5-5663-48b1-a964-66d312f336ee",
   "metadata": {},
   "source": [
    "## Creating Custom CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284c37ef-52d5-45e7-b8ae-9763794da0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "            #nn.BatchNorm2d()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear = nn.Linear(128*5*11, 25)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(128*5*11, 26)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear(x)\n",
    "        #predictions = self.softmax(logits)\n",
    "        predictions = logits\n",
    "        return predictions\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6413194-205a-47df-91e1-cf5b087f4412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 66, 159]             160\n",
      "              ReLU-2          [-1, 16, 66, 159]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 79]               0\n",
      "            Conv2d-4           [-1, 32, 35, 81]           4,640\n",
      "              ReLU-5           [-1, 32, 35, 81]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 40]               0\n",
      "            Conv2d-7           [-1, 64, 19, 42]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 42]               0\n",
      "         MaxPool2d-9            [-1, 64, 9, 21]               0\n",
      "           Conv2d-10          [-1, 128, 11, 23]          73,856\n",
      "             ReLU-11          [-1, 128, 11, 23]               0\n",
      "        MaxPool2d-12           [-1, 128, 5, 11]               0\n",
      "          Flatten-13                 [-1, 7040]               0\n",
      "          Dropout-14                 [-1, 7040]               0\n",
      "           Linear-15                   [-1, 26]         183,066\n",
      "================================================================\n",
      "Total params: 280,218\n",
      "Trainable params: 280,218\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 5.96\n",
      "Params size (MB): 1.07\n",
      "Estimated Total Size (MB): 7.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn = AudioCNN()\n",
    "summary(cnn.cuda(), (1, 64, 157))\n",
    "#print(cnn.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f41b8b-73ec-45f9-b24e-872379c5b507",
   "metadata": {},
   "source": [
    "## Define Training Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666ac93a-4cc6-434b-8247-c7b49011d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "\n",
    "def train_single_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        prediction=model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, data_loader, loss_fn, optimizer, device)        \n",
    "        print(\"----------------------\")\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8b068-16ed-4c1a-b32a-049c4645b658",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa27ca27-495b-4914-a34b-75dfb24d3c58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "There are 350 samples in the dataset.\n",
      "1\n",
      "Epoch 1\n",
      "loss: 30.611759185791016\n",
      "----------------------\n",
      "Epoch 2\n",
      "loss: 8.380337715148926\n",
      "----------------------\n",
      "Epoch 3\n",
      "loss: 2.9110939502716064\n",
      "----------------------\n",
      "Epoch 4\n",
      "loss: 2.344248056411743\n",
      "----------------------\n",
      "Epoch 5\n",
      "loss: 2.192189931869507\n",
      "----------------------\n",
      "Epoch 6\n",
      "loss: 2.0632848739624023\n",
      "----------------------\n",
      "Epoch 7\n",
      "loss: 2.037885904312134\n",
      "----------------------\n",
      "Epoch 8\n",
      "loss: 1.9379204511642456\n",
      "----------------------\n",
      "Epoch 9\n",
      "loss: 1.8714059591293335\n",
      "----------------------\n",
      "Epoch 10\n",
      "loss: 1.749122142791748\n",
      "----------------------\n",
      "Epoch 11\n",
      "loss: 1.6265904903411865\n",
      "----------------------\n",
      "Epoch 12\n",
      "loss: 1.5361310243606567\n",
      "----------------------\n",
      "Epoch 13\n",
      "loss: 1.444130539894104\n",
      "----------------------\n",
      "Epoch 14\n",
      "loss: 1.3229265213012695\n",
      "----------------------\n",
      "Epoch 15\n",
      "loss: 1.2557774782180786\n",
      "----------------------\n",
      "Epoch 16\n",
      "loss: 1.1826640367507935\n",
      "----------------------\n",
      "Epoch 17\n",
      "loss: 1.1635364294052124\n",
      "----------------------\n",
      "Epoch 18\n",
      "loss: 1.0608009099960327\n",
      "----------------------\n",
      "Epoch 19\n",
      "loss: 0.9477599859237671\n",
      "----------------------\n",
      "Epoch 20\n",
      "loss: 0.9479963183403015\n",
      "----------------------\n",
      "Epoch 21\n",
      "loss: 0.843799352645874\n",
      "----------------------\n",
      "Epoch 22\n",
      "loss: 0.8019566535949707\n",
      "----------------------\n",
      "Epoch 23\n",
      "loss: 0.7632493376731873\n",
      "----------------------\n",
      "Epoch 24\n",
      "loss: 0.7358818054199219\n",
      "----------------------\n",
      "Epoch 25\n",
      "loss: 0.7162356972694397\n",
      "----------------------\n",
      "Epoch 26\n",
      "loss: 0.6801667213439941\n",
      "----------------------\n",
      "Epoch 27\n",
      "loss: 0.6724883913993835\n",
      "----------------------\n",
      "Epoch 28\n",
      "loss: 0.5856559872627258\n",
      "----------------------\n",
      "Epoch 29\n",
      "loss: 0.5780416131019592\n",
      "----------------------\n",
      "Epoch 30\n",
      "loss: 0.531927764415741\n",
      "----------------------\n",
      "Epoch 31\n",
      "loss: 0.5982701182365417\n",
      "----------------------\n",
      "Epoch 32\n",
      "loss: 0.4994151294231415\n",
      "----------------------\n",
      "Epoch 33\n",
      "loss: 0.46505722403526306\n",
      "----------------------\n",
      "Epoch 34\n",
      "loss: 0.458589106798172\n",
      "----------------------\n",
      "Epoch 35\n",
      "loss: 0.45627519488334656\n",
      "----------------------\n",
      "Epoch 36\n",
      "loss: 0.680600643157959\n",
      "----------------------\n",
      "Epoch 37\n",
      "loss: 0.45098990201950073\n",
      "----------------------\n",
      "Epoch 38\n",
      "loss: 0.4494496285915375\n",
      "----------------------\n",
      "Epoch 39\n",
      "loss: 0.4042632281780243\n",
      "----------------------\n",
      "Epoch 40\n",
      "loss: 0.4097323417663574\n",
      "----------------------\n",
      "Epoch 41\n",
      "loss: 0.4027256369590759\n",
      "----------------------\n",
      "Epoch 42\n",
      "loss: 0.36599159240722656\n",
      "----------------------\n",
      "Epoch 43\n",
      "loss: 0.3535960912704468\n",
      "----------------------\n",
      "Epoch 44\n",
      "loss: 0.3351992666721344\n",
      "----------------------\n",
      "Epoch 45\n",
      "loss: 0.33724692463874817\n",
      "----------------------\n",
      "Epoch 46\n",
      "loss: 0.3842523396015167\n",
      "----------------------\n",
      "Epoch 47\n",
      "loss: 0.3231225907802582\n",
      "----------------------\n",
      "Epoch 48\n",
      "loss: 0.32775428891181946\n",
      "----------------------\n",
      "Epoch 49\n",
      "loss: 0.32015931606292725\n",
      "----------------------\n",
      "Epoch 50\n",
      "loss: 0.33351650834083557\n",
      "----------------------\n",
      "Finished Training\n",
      "Trained cnn saved\n"
     ]
    }
   ],
   "source": [
    "TRAIN_LABELS_FILE=\"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/train/labels.txt\"\n",
    "TRAIN_WAV_DIR=\"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/train\"\n",
    "\n",
    "BATCH_SIZE=128\n",
    "EPOCHS=50\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "#5 second audio signals\n",
    "SAMPLE_RATE=16000\n",
    "NUM_SAMPLES= 80000\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#instantiate transformer\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "\n",
    "#instantiate dataset object\n",
    "train_dataset = AudioDataset(TRAIN_WAV_DIR, mel_spectogram, SAMPLE_RATE, NUM_SAMPLES, device, TRAIN_LABELS_FILE)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} samples in the dataset.\")\n",
    "signal, label = train_dataset[2]\n",
    "print(label)\n",
    "#print(signal.shape)\n",
    "\n",
    "#instatnitate dataloader\n",
    "train_dataloader = create_data_loader(train_dataset, BATCH_SIZE)\n",
    "\n",
    "cnn = AudioCNN().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train(cnn, train_dataloader, loss_fn, optimizer, device, EPOCHS)\n",
    "\n",
    "torch.save(cnn.state_dict(), \"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/saved_models/program3_2.pth\")\n",
    "print(\"Trained cnn saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb871c-1514-4c99-82a8-377c6d8ce2fe",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f636371-03dc-44eb-a7ec-ca5cbaf99e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, input, target):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        predicted = int(predictions[0].argmax(0))\n",
    "        expected = target\n",
    "    return predicted, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0297006-f8d0-4472-971d-161d90dd585c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: '25', Expected: '23'\n",
      "Predicted: '15', Expected: '15'\n",
      "Predicted: '3', Expected: '6'\n",
      "Predicted: '22', Expected: '16'\n",
      "Predicted: '20', Expected: '20'\n",
      "Predicted: '4', Expected: '5'\n",
      "Predicted: '7', Expected: '14'\n",
      "Predicted: '2', Expected: '4'\n",
      "Predicted: '1', Expected: '1'\n",
      "Predicted: '25', Expected: '25'\n",
      "Predicted: '19', Expected: '22'\n",
      "Predicted: '4', Expected: '21'\n",
      "Predicted: '8', Expected: '9'\n",
      "Predicted: '14', Expected: '7'\n",
      "Predicted: '5', Expected: '10'\n",
      "Predicted: '1', Expected: '8'\n",
      "Predicted: '10', Expected: '10'\n",
      "Predicted: '25', Expected: '18'\n",
      "Predicted: '8', Expected: '8'\n",
      "Predicted: '24', Expected: '7'\n",
      "Predicted: '19', Expected: '19'\n",
      "Predicted: '12', Expected: '12'\n",
      "Predicted: '7', Expected: '2'\n",
      "Predicted: '7', Expected: '3'\n",
      "Predicted: '6', Expected: '6'\n",
      "Predicted: '1', Expected: '8'\n",
      "Predicted: '5', Expected: '19'\n",
      "Predicted: '21', Expected: '21'\n",
      "Predicted: '16', Expected: '2'\n",
      "Predicted: '4', Expected: '16'\n",
      "Predicted: '1', Expected: '1'\n",
      "Predicted: '1', Expected: '23'\n",
      "Predicted: '13', Expected: '13'\n",
      "Predicted: '20', Expected: '2'\n",
      "Predicted: '17', Expected: '17'\n",
      "Predicted: '15', Expected: '12'\n",
      "Predicted: '23', Expected: '9'\n",
      "Predicted: '25', Expected: '25'\n",
      "Predicted: '24', Expected: '24'\n",
      "Predicted: '24', Expected: '18'\n",
      "Predicted: '16', Expected: '16'\n",
      "Predicted: '13', Expected: '5'\n",
      "Predicted: '24', Expected: '9'\n",
      "Predicted: '20', Expected: '20'\n",
      "Predicted: '20', Expected: '15'\n",
      "Predicted: '13', Expected: '13'\n",
      "Predicted: '7', Expected: '11'\n",
      "Predicted: '24', Expected: '11'\n",
      "Predicted: '25', Expected: '25'\n",
      "Predicted: '3', Expected: '13'\n",
      "Predicted: '4', Expected: '4'\n",
      "Predicted: '21', Expected: '22'\n",
      "Predicted: '3', Expected: '6'\n",
      "Predicted: '21', Expected: '21'\n",
      "Predicted: '9', Expected: '23'\n",
      "Predicted: '14', Expected: '15'\n",
      "Predicted: '2', Expected: '14'\n",
      "Predicted: '15', Expected: '5'\n",
      "Predicted: '21', Expected: '18'\n",
      "Predicted: '19', Expected: '17'\n",
      "Predicted: '20', Expected: '24'\n",
      "Predicted: '1', Expected: '1'\n",
      "Predicted: '17', Expected: '17'\n",
      "Predicted: '24', Expected: '24'\n",
      "Predicted: '14', Expected: '7'\n",
      "Predicted: '12', Expected: '12'\n",
      "Predicted: '19', Expected: '11'\n",
      "Predicted: '19', Expected: '19'\n",
      "Predicted: '21', Expected: '22'\n",
      "Predicted: '13', Expected: '4'\n",
      "Predicted: '7', Expected: '20'\n",
      "Predicted: '1', Expected: '3'\n",
      "Predicted: '15', Expected: '10'\n",
      "Predicted: '3', Expected: '3'\n",
      "Predicted: '21', Expected: '14'\n",
      "True samples: 27, All samples: 75, Percentage: 0.36\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "#load the model\n",
    "cnn = AudioCNN()\n",
    "state_dict = torch.load(\"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/saved_models/program3_2.pth\")\n",
    "cnn.load_state_dict(state_dict)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "#load the validation dataset\n",
    "VAL_LABELS_FILE=\"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/val/labels.txt\"\n",
    "VAL_WAV_DIR=\"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/val\"\n",
    "\n",
    "#5 second audio signals\n",
    "SAMPLE_RATE=16000\n",
    "NUM_SAMPLES= 80000\n",
    "\n",
    "#instantiate transformer\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "\n",
    "#instantiate dataset object\n",
    "val_dataset = AudioDataset(VAL_WAV_DIR, mel_spectogram, SAMPLE_RATE, NUM_SAMPLES, device, VAL_LABELS_FILE)\n",
    "\n",
    "#make inferences on labels\n",
    "true = 0\n",
    "for i in range(len(val_dataset)):\n",
    "    input, target = val_dataset[i][0], val_dataset[i][1]\n",
    "    #print(input)\n",
    "    #print(target)\n",
    "    input.unsqueeze_(0)\n",
    "    predicted, expected = predict(cnn, input, target)\n",
    "    print(f\"Predicted: '{predicted}', Expected: '{expected}'\")\n",
    "    if predicted == expected:\n",
    "        true+=1\n",
    "\n",
    "\n",
    "print(f\"True samples: {true}, All samples: {len(val_dataset)}, Percentage: {true/len(val_dataset)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1563d8b-4fa1-4a8e-81d4-5e7530e4f861",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04fa82cd-e99b-4491-82e3-e54dadf6546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, input):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        predicted = int(predictions[0].argmax(0))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "474df7eb-81bd-45b7-a026-b6432e830306",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_PATH = \"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/predictions.txt\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "#load the model\n",
    "cnn = AudioCNN()\n",
    "state_dict = torch.load(\"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/saved_models/program3_2.pth\")\n",
    "cnn.load_state_dict(state_dict)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "#5 second audio signals\n",
    "SAMPLE_RATE=16000\n",
    "NUM_SAMPLES= 80000\n",
    "\n",
    "#instantiate transformer\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "\n",
    "#load the test dataset\n",
    "TEST_WAV_DIR=\"/WAVE/archive/users/emitchell/CSEN342W25/CSEN342P3/test\"\n",
    "test_dataset = AudioDataset(TEST_WAV_DIR, mel_spectogram, SAMPLE_RATE, NUM_SAMPLES, device)\n",
    "\n",
    "format_lines = []\n",
    "\n",
    "#print(test_dataset[i])\n",
    "\n",
    "for i in range(75):\n",
    "    input = test_dataset[i]\n",
    "    input.unsqueeze_(0)\n",
    "    predicted = predict_test(cnn, input)\n",
    "    format_lines.append(str(predicted))\n",
    "\n",
    "with open(PREDICTIONS_PATH, \"w\") as output_file:\n",
    "        output_file.write(\"\\n\".join(format_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be27c2-5fed-4574-b134-5d93a31274a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
