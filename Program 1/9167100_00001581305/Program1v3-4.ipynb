{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7145428a-6b54-4725-8c39-7c0e4e320e34",
   "metadata": {},
   "source": [
    "# Program 1: Peptide Classification\n",
    "\n",
    "- Create feed-forward neural networks and train them using your own codes and\n",
    "frameworks.\n",
    "- Experiment with different feature extraction techniques.\n",
    "- Think about dealing with imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a27f4841-8a3c-4a52-9f6b-a194c27d78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c1187-51fd-44a6-b5bf-3dd6627ac62c",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "We import the training and test data from the given files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51af09f7-2c25-49bb-a180-1081ca293c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_training_data(file_path):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            line = line.split(\"\\t\")\n",
    "            #print(line)\n",
    "            y_train.append(int(line[0]))\n",
    "            x_train.append(line[1])\n",
    "    return x_train, y_train\n",
    "\n",
    "def parse_testing_data(file_path):\n",
    "    x_test = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            #print(line)\n",
    "            x_test.append(line)\n",
    "    \n",
    "    return x_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f73fa424-0ff1-4aa1-ae3c-e21211d077f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input, Y_train_input = parse_training_data('train.dat')\n",
    "X_test_input = parse_testing_data('test.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b2ebc-8bdb-43f2-9484-2267eed5729e",
   "metadata": {},
   "source": [
    "## Generate KMERs\n",
    "\n",
    "Generate kmers for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ea22b38-1564-4037-a650-b06ef0cd8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer(seq, k):\n",
    "    return [seq[i:i+k] for i in range(0, len(seq)-k+1)]\n",
    "\n",
    "def kmers(seq, k):\n",
    "    kmers = []\n",
    "    for i in range(1,k):\n",
    "        kmers.extend(kmer(seq, k=i))\n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "824ee722-ebe8-4d9e-8b03-7dc178507117",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kmers = [kmers(seq, k=3) for seq in X_train_input]\n",
    "X_test_kmers = [kmers(seq, k=3) for seq in X_test_input]\n",
    "#print(len(X_train_kmers[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3791de8-eb9f-4180-b5f7-765ae288fd72",
   "metadata": {},
   "source": [
    "## Encode KMERs\n",
    "\n",
    "Encode kmers into matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ce8940-e179-438d-9935-80e50e83449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kmer_mapping(train_kmers, test_kmers):\n",
    "    mapping = {}\n",
    "    for peptide in train_kmers+test_kmers:\n",
    "        for amino_acid in peptide:\n",
    "            if amino_acid not in mapping:\n",
    "                mapping[amino_acid] = len(mapping)\n",
    "    return mapping\n",
    "\n",
    "def encoding_matrix(data, mapping):\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(mapping)\n",
    "    \n",
    "    matrix = np.zeros((num_rows, num_cols), dtype=float)\n",
    "    \n",
    "    for i, peptide in enumerate(data):\n",
    "        unique_elements, counts = np.unique(peptide, return_counts=True)\n",
    "        \n",
    "        for kmer, count in zip(unique_elements, counts):\n",
    "            if kmer in mapping:\n",
    "                matrix[i, mapping[kmer]] = count\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "714ce466-9d36-4488-8a5b-fa52ea06d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "mapping = generate_kmer_mapping(X_train_kmers, X_test_kmers)\n",
    "\n",
    "X_train_encoded = encoding_matrix(X_train_kmers, mapping)\n",
    "X_test_encoded = encoding_matrix(X_test_kmers, mapping)\n",
    "\n",
    "X_train_encoded = np.array(X_train_encoded)\n",
    "print(len(X_train_encoded[0]))\n",
    "X_test_encoded = np.array(X_test_encoded)\n",
    "Y_train_input = np.array(Y_train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a718893-c666-487c-ba1f-e94abdf49396",
   "metadata": {},
   "source": [
    "## Balance Dataset\n",
    "\n",
    "There is an imbalance of 1 and -1 samples\n",
    "Need to make copies of 1 samples to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4f7d8bf-eb28-46f1-a0c4-17c27a24db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    }
   ],
   "source": [
    "ones = X_train_encoded[Y_train_input == 1]\n",
    "neg_ones = X_train_encoded[Y_train_input == -1]\n",
    "\n",
    "#create copies\n",
    "num_copies = int((neg_ones.shape[0] - ones.shape[0]) / 2)\n",
    "print(num_copies)\n",
    "copies = ones[np.random.randint(ones.shape[0], size=num_copies)]\n",
    "\n",
    "#add copies to labels and values\n",
    "balanced_X = np.vstack((X_train_encoded, copies))\n",
    "balanced_Y = np.hstack((Y_train_input, np.ones(num_copies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572df19-67e2-4483-b2c3-2b9477bfb97f",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa6f30f-738c-49b8-92b7-7f74fb32aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = np.arange(balanced_X.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "X_train_shuffled = balanced_X[shuffle]\n",
    "Y_train_shuffled = balanced_Y[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf81b5-0241-4c19-981e-e644bb6a5309",
   "metadata": {},
   "source": [
    "## Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48962ed8-0ecf-49a5-986c-927b04f9a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(X_train_shuffled.shape[0] / 2)\n",
    "\n",
    "#Training Set\n",
    "X_train = X_train_shuffled[half:]\n",
    "Y_train = Y_train_shuffled[half:]\n",
    "#print(len(X_train))\n",
    "\n",
    "#Validation Set\n",
    "X_test_validation = X_train_shuffled[:half]\n",
    "Y_test_validation = Y_train_shuffled[:half]\n",
    "\n",
    "#Test Set\n",
    "X_test = X_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa20e97-6703-471f-a4b8-e2caa0d7a1ff",
   "metadata": {},
   "source": [
    "## Methods for Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c558f02-a935-4b37-9ca8-385a7a5a0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "000d4487-e443-4380-bd6a-b05ab51540e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer(Layer):\n",
    "    def __init__(self, input, output):\n",
    "        self.W = np.random.rand(input, output) - 0.5\n",
    "        self.b = np.random.rand(1, output) - 0.5\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.input, self.W) + self.b\n",
    "\n",
    "    def backward(self, loss, learning_rate):\n",
    "        db = loss\n",
    "        gradient = np.dot(loss, self.W.T)\n",
    "        dW = np.dot(self.input.T, loss)\n",
    "\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e01d008d-a898-4771-9913-46986257a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctionLayer(Layer):\n",
    "    def __init__(self, function, function_derivative):\n",
    "        self.function = function\n",
    "        self.function_derivative = function_derivative\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.function(input)\n",
    "\n",
    "    def backward(self, loss, learning_rate):\n",
    "        return self.function_derivative(self.input) * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "870fa3ee-3419-4f40-a9d6-228985de661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1056af1a-8127-4683-b4b4-d9ef9f2a8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(y, y_hat):\n",
    "    return np.mean(np.power(y - y_hat, 2))\n",
    "\n",
    "def loss_mse_derivative(y, y_hat):\n",
    "    return 2 * (y_hat - y) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b394c3a-061f-4db2-9854-9e057853bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss_function = loss_mse\n",
    "        self.loss_derivative = loss_mse_derivative\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            losses = 0\n",
    "            for i in range(len(X_train)):\n",
    "                output = X_train[i]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                losses += self.loss_function(Y_train[i], output)\n",
    "\n",
    "                loss = self.loss_derivative(Y_train[i], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    loss = layer.backward(loss, learning_rate)\n",
    "\n",
    "            average_loss = losses / len(X_train)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss {average_loss:.8f}\")\n",
    "            \n",
    "    def predict(self, X):\n",
    "        outputs = []\n",
    "        y_hat = []\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            output = X[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward(output)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        outputs = np.array([output[0][0] for output in outputs])\n",
    "        \n",
    "        y_hat = np.where(outputs >= 0.5, 1, -1)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d41de-4a55-4555-ae93-4c230e97c9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db349fa0-0792-495e-9906-f2d19f099fa5",
   "metadata": {},
   "source": [
    "## Main Method\n",
    "\n",
    "Create model layers, train model, and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be5a738a-1fb5-4fbb-b81f-8f5abd969a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.27323106\n",
      "Epoch 20, Loss 0.00020241\n",
      "Epoch 40, Loss 0.00007459\n",
      "Epoch 60, Loss 0.00004519\n",
      "Epoch 80, Loss 0.00003221\n",
      "Epoch 100, Loss 0.00002493\n",
      "Epoch 120, Loss 0.00002028\n",
      "Epoch 140, Loss 0.00001707\n",
      "Epoch 160, Loss 0.00001472\n",
      "Epoch 180, Loss 0.00001292\n",
      "Epoch 200, Loss 0.00001151\n",
      "Epoch 220, Loss 0.00001037\n",
      "Epoch 240, Loss 0.00000943\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #feed in each sample one at a time -> add dimension to data sets\n",
    "    X_train_matrix = np.expand_dims(X_train, axis=1)\n",
    "    Y_train_matrix = np.expand_dims(Y_train, axis=1)\n",
    "\n",
    "    X_test_validation_matrix = np.expand_dims(X_test_validation, axis=1)\n",
    "\n",
    "    X_test_matrix = np.expand_dims(X_test, axis=1)\n",
    "    \n",
    "    ff = FeedForward()\n",
    "    ff.layers.append(NeuralLayer(436, 64))\n",
    "    ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative)) \n",
    "    ff.layers.append(NeuralLayer(64, 32))\n",
    "    ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative))\n",
    "    ff.layers.append(NeuralLayer(32, 16))\n",
    "    ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative))\n",
    "    ff.layers.append(NeuralLayer(16, 1))\n",
    "    ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative))\n",
    "    \n",
    "    ff.train(X_train_matrix, Y_train_matrix, epochs=210, learning_rate=0.01)\n",
    "\n",
    "    #prediction on validation set\n",
    "    Y_hat_test_validation = ff.predict(X_test_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524d1aa9-3771-4f1d-aef3-5d25d59d24d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC:  0.9563547080375059\n"
     ]
    }
   ],
   "source": [
    "def mcc(y, y_hat):\n",
    "\n",
    "    TP = np.sum((y == 1) & (y_hat == 1)) / len(y)\n",
    "    TN = np.sum((y == -1) & (y_hat == -1)) / len(y)\n",
    "    FP = np.sum((y == -1) & (y_hat == 1)) / len(y)\n",
    "    FN = np.sum((y == 1) & (y_hat == -1)) / len(y)\n",
    "    \n",
    "    return ((TP * TN) - (FP * FN)) / (np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n",
    "\n",
    "print(\"MCC: \", mcc(Y_test_validation, Y_hat_test_validation))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e51f0a-8b44-45f7-bcc0-dd625f45ddf7",
   "metadata": {},
   "source": [
    "## Generate Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa15c5be-8b9f-4323-87d4-86793675ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "Y_hat_test = ff.predict(X_test_matrix)\n",
    "\n",
    "#generate test predictions\n",
    "np.savetxt(\"predictions.dat\", Y_hat_test, fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01984c-8d2f-41e1-bec0-108595066b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi25",
   "language": "python",
   "name": "342wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
