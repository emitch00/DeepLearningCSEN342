{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7145428a-6b54-4725-8c39-7c0e4e320e34",
   "metadata": {},
   "source": [
    "# Program 1: Peptide Classification - using Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27f4841-8a3c-4a52-9f6b-a194c27d78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c1187-51fd-44a6-b5bf-3dd6627ac62c",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "We import the training and test data from the given files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51af09f7-2c25-49bb-a180-1081ca293c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_training_data(file_path):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            line = line.split(\"\\t\")\n",
    "            #print(line)\n",
    "            y_train.append(int(line[0]))\n",
    "            x_train.append(line[1])\n",
    "    return x_train, y_train\n",
    "\n",
    "def parse_testing_data(file_path):\n",
    "    x_test = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            #print(line)\n",
    "            x_test.append(line)\n",
    "    \n",
    "    return x_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73fa424-0ff1-4aa1-ae3c-e21211d077f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input, Y_train_input = parse_training_data('train.dat')\n",
    "X_test_input = parse_testing_data('test.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b2ebc-8bdb-43f2-9484-2267eed5729e",
   "metadata": {},
   "source": [
    "## Generate KMERs\n",
    "\n",
    "Generate kmers for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea22b38-1564-4037-a650-b06ef0cd8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer(seq, k):\n",
    "    return [seq[i:i+k] for i in range(0, len(seq)-k+1)]\n",
    "\n",
    "def kmers(seq, k):\n",
    "    kmers = []\n",
    "    for i in range(1,k):\n",
    "        kmers.extend(kmer(seq, k=i))\n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824ee722-ebe8-4d9e-8b03-7dc178507117",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kmers = [kmers(seq, k=3) for seq in X_train_input]\n",
    "X_test_kmers = [kmers(seq, k=3) for seq in X_test_input]\n",
    "#print(X_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3791de8-eb9f-4180-b5f7-765ae288fd72",
   "metadata": {},
   "source": [
    "## Encode KMERs\n",
    "\n",
    "Encode kmers into matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ce8940-e179-438d-9935-80e50e83449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kmer_mapping(train_kmers, test_kmers):\n",
    "    mapping = {}\n",
    "    for peptide in train_kmers+test_kmers:\n",
    "        for amino_acid in peptide:\n",
    "            if amino_acid not in mapping:\n",
    "                mapping[amino_acid] = len(mapping)\n",
    "    return mapping\n",
    "\n",
    "def encoding_matrix(data, mapping):\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(mapping)\n",
    "    \n",
    "    matrix = np.zeros((num_rows, num_cols), dtype=float)\n",
    "    \n",
    "    for i, peptide in enumerate(data):\n",
    "        unique_elements, counts = np.unique(peptide, return_counts=True)\n",
    "        \n",
    "        for kmer, count in zip(unique_elements, counts):\n",
    "            if kmer in mapping:\n",
    "                matrix[i, mapping[kmer]] = count\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714ce466-9d36-4488-8a5b-fa52ea06d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = generate_kmer_mapping(X_train_kmers, X_test_kmers)\n",
    "\n",
    "X_train_encoded = encoding_matrix(X_train_kmers, mapping)\n",
    "X_test_encoded = encoding_matrix(X_test_kmers, mapping)\n",
    "\n",
    "X_train_encoded = np.array(X_train_encoded)\n",
    "X_test_encoded = np.array(X_test_encoded)\n",
    "Y_train_input = np.array(Y_train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a718893-c666-487c-ba1f-e94abdf49396",
   "metadata": {},
   "source": [
    "## Balance Dataset\n",
    "\n",
    "There is an imbalance of 1 and -1 samples\n",
    "Need to make copies of 1 samples to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f7d8bf-eb28-46f1-a0c4-17c27a24db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    }
   ],
   "source": [
    "ones = X_train_encoded[Y_train_input == 1]\n",
    "neg_ones = X_train_encoded[Y_train_input == -1]\n",
    "\n",
    "#create copies\n",
    "num_copies = int((neg_ones.shape[0] - ones.shape[0]) / 2)\n",
    "print(num_copies)\n",
    "copies = ones[np.random.randint(ones.shape[0], size=num_copies)]\n",
    "\n",
    "#add copies to labels and values\n",
    "balanced_X = np.vstack((X_train_encoded, copies))\n",
    "balanced_Y = np.hstack((Y_train_input, np.ones(num_copies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572df19-67e2-4483-b2c3-2b9477bfb97f",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fa6f30f-738c-49b8-92b7-7f74fb32aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = np.arange(balanced_X.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "X_train_shuffled = balanced_X[shuffle]\n",
    "Y_train_shuffled = balanced_Y[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab6d08-fc76-46a3-b3c4-2f2ade1a612a",
   "metadata": {},
   "source": [
    "## Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4305b267-d1b9-4581-b095-14c9de19455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(X_train_shuffled.shape[0] / 2)\n",
    "\n",
    "#Training Set\n",
    "X_train = X_train_shuffled[half:]\n",
    "Y_train = Y_train_shuffled[half:]\n",
    "\n",
    "#Validation Set\n",
    "X_test_validation = X_train_shuffled[:half]\n",
    "Y_test_validation = Y_train_shuffled[:half]\n",
    "\n",
    "#Test Set\n",
    "X_test = X_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa20e97-6703-471f-a4b8-e2caa0d7a1ff",
   "metadata": {},
   "source": [
    "## Methods for Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e4fbfcd-84b0-458a-ad5a-716c4d318c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyper-parameters \n",
    "epochs = 350\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a6d41de-4a55-4555-ae93-4c230e97c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTorch:\n",
    "    def __init__(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        #self.loss_derivative = nn.MSELoss\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs, learning_rate):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            losses = 0\n",
    "            for i in range(len(X_train)):\n",
    "                output = X_train[i]\n",
    "                for layer in self.layers:\n",
    "                    output = layer(output)\n",
    "\n",
    "                #loss\n",
    "                loss = self.loss_function(output, Y_train[i].unsqueeze(0))\n",
    "                losses += loss.item()\n",
    "\n",
    "                # Backwards\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            average_loss = losses / len(X_train)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss {average_loss:.8f}\")\n",
    "\n",
    "    #def train(self, X_train, Y_train, epochs, learning_rate):\n",
    "    #    optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    #    for epoch in range(epochs):\n",
    "    #        total_loss = 0\n",
    "\n",
    "    #        for i in range(len(X_train)):\n",
    "                # Forwards\n",
    "    #            output = X_train[i]\n",
    "    #            for layer in self.layers:\n",
    "    #                output = layer(output)\n",
    "\n",
    "                #loss\n",
    "    #            loss = self.loss_function(output, Y_train[i].unsqueeze(0))\n",
    "    #            total_loss += loss.item()\n",
    "\n",
    "                # Backwards\n",
    "     #           optimizer.zero_grad()\n",
    "     #           loss.backward()\n",
    "     #           optimizer.step()\n",
    "\n",
    "      #      average_loss = total_loss / len(X_train)\n",
    "      #      if epoch % 20 == 0:\n",
    "      #          print(f\"Epoch {epoch}, Loss {average_loss:.8f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = []\n",
    "        y_hat = []\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            output = X[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "        outputs = torch.stack([output[0] for output in outputs]).detach()\n",
    "        y_hat = torch.where(outputs >= 0.5, 1, -1)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c3e2503-e5e5-4564-87b9-ea393e6258b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Layer(\n",
       "    (linear): Linear(in_features=436, out_features=64, bias=True)\n",
       "  )\n",
       "  (1): Layer(\n",
       "    (linear): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (2): Layer(\n",
       "    (linear): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (3): Layer(\n",
       "    (linear): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = FeedForwardTorch()\n",
    "model.layers.append(Layer(436, 64))\n",
    "model.layers.append(Layer(64, 32))\n",
    "model.layers.append(Layer(32, 16))\n",
    "model.layers.append(Layer(16, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db349fa0-0792-495e-9906-f2d19f099fa5",
   "metadata": {},
   "source": [
    "## Main Method\n",
    "\n",
    "Create model layers, train model, and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be5a738a-1fb5-4fbb-b81f-8f5abd969a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.40114225\n",
      "Epoch 20, Loss 0.16741812\n",
      "Epoch 40, Loss 0.15037435\n",
      "Epoch 60, Loss 0.13609270\n",
      "Epoch 80, Loss 0.11664521\n",
      "Epoch 100, Loss 0.10003806\n",
      "Epoch 120, Loss 0.09154387\n",
      "Epoch 140, Loss 0.08565791\n",
      "Epoch 160, Loss 0.08253271\n",
      "Epoch 180, Loss 0.07942265\n",
      "Epoch 200, Loss 0.07709282\n",
      "Epoch 220, Loss 0.07573644\n",
      "Epoch 240, Loss 0.07438206\n",
      "Epoch 260, Loss 0.07378084\n",
      "Epoch 280, Loss 0.07349369\n",
      "Epoch 300, Loss 0.07115531\n",
      "Epoch 320, Loss 0.06967966\n",
      "Epoch 340, Loss 0.06842111\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #feed in each sample one at a time -> add dimension to data sets\n",
    "    X_train_matrix = np.expand_dims(X_train, axis=1)\n",
    "    Y_train_matrix = np.expand_dims(Y_train, axis=1)\n",
    "\n",
    "    X_test_validation_matrix = np.expand_dims(X_test_validation, axis=1)\n",
    "\n",
    "    X_test_matrix = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_train_matrix).float()\n",
    "    Y_tensor = torch.from_numpy(Y_train_matrix).float()\n",
    "\n",
    "    X_test_validation_tensor = torch.from_numpy(X_test_validation_matrix).float()\n",
    "\n",
    "    X_test_tensor = torch.from_numpy(X_test_matrix).float()\n",
    "\n",
    "    \n",
    "    #print(X_tensor)\n",
    "    #print(Y_tensor)\n",
    "\n",
    "    \n",
    "    #ff = FeedForward()\n",
    "    #ff.layers.append(DenseLayer(436, 64))\n",
    "    #ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative)) \n",
    "    #ff.layers.append(DenseLayer(64, 32))\n",
    "    #ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative))\n",
    "    #ff.layers.append(DenseLayer(32, 16))\n",
    "    #ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative))\n",
    "    #ff.layers.append(DenseLayer(16, 1))\n",
    "    #ff.layers.append(ActivationFunctionLayer(tanh, tanh_derivative))\n",
    "    \n",
    "    #ff.train(X_train_matrix, Y_train_matrix, epochs=200, learning_rate=0.01)\n",
    "\n",
    "    model.train(X_tensor, Y_tensor, epochs, learning_rate)\n",
    "\n",
    "    #prediction on validation set\n",
    "    Y_hat_test_validation = model.predict(X_test_validation_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6fdbe514-68b2-4039-962f-2f81d005a158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC:  0.4163026153212167\n"
     ]
    }
   ],
   "source": [
    "def mcc(y, y_hat):\n",
    "\n",
    "    TP = np.sum((y == 1) & (y_hat == 1)) / len(y)\n",
    "    #print(TP)\n",
    "    TN = np.sum((y == -1) & (y_hat == -1)) / len(y)\n",
    "    #print(TN)\n",
    "    FP = np.sum((y == -1) & (y_hat == 1)) / len(y)\n",
    "    #print(FP)\n",
    "    FN = np.sum((y == 1) & (y_hat == -1)) / len(y)\n",
    "    #print(FN)\n",
    "\n",
    "    #print(((TP * TN) - (FP * FN)))\n",
    "\n",
    "    #print(np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n",
    "    \n",
    "    return ((TP * TN) + (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "\n",
    "Y_hat_array = Y_hat_test_validation.numpy()\n",
    "\n",
    "#Y_test_validation_matrix = np.expand_dims(Y_test_validation, axis=1)\n",
    "#Y_test_validation_tensor = torch.from_numpy(Y_test_validation_matrix).float()\n",
    "\n",
    "print(\"MCC: \", mcc(Y_test_validation, Y_hat_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e51f0a-8b44-45f7-bcc0-dd625f45ddf7",
   "metadata": {},
   "source": [
    "## Generate Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15c5be-8b9f-4323-87d4-86793675ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "#Y_hat_test = ff.predict(X_test_tensor)\n",
    "\n",
    "#generate test predictions\n",
    "#np.savetxt(\"predictions.dat\", Y_hat_test, fmt=\"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi25",
   "language": "python",
   "name": "342wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
